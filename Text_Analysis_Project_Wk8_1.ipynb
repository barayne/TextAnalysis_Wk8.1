{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Analysis_Project_Wk8.1",
      "provenance": [],
      "collapsed_sections": [
        "enxDwtXPZTpT",
        "85tygFqJZ0Xw",
        "96Uz3PxJZ6E7",
        "YlHYpKxfHRJ4",
        "LWiaaYPCnsiC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH-sF6hRXeBZ"
      },
      "source": [
        "<font color=\"#4b76b7\">To start practicing, you will need to make a copy of it. Go to File > Save a Copy in Drive. You can then use the new copy that will appear in the new tab.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC3nTnLyZNnv"
      },
      "source": [
        "# AfterWork Data Science: Getting Started with NLP Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enxDwtXPZTpT"
      },
      "source": [
        "### Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz7plWFhaQ9R"
      },
      "source": [
        "# Importing the required libraries\n",
        "# ---\n",
        "# \n",
        "import pandas as pd # library for data manipulation\n",
        "import numpy as np  # librariy for scientific computations\n",
        "import re           # regex library to perform text preprocessing\n",
        "import string       # library to work with strings\n",
        "import nltk         # library for natural language processing\n",
        "import scipy        # scientific conputing "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85tygFqJZ0Xw"
      },
      "source": [
        "### 1. Importing our Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_d2EpfjafP3"
      },
      "source": [
        "# Question: Given a new tweets, create a sentiment analysis model that will \n",
        "# predict whether a tweet will contain positive or negative sentiment.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/31kqByD \n",
        "# ---\n",
        "#\n",
        "df = pd.read_csv('https://bit.ly/31kqByD', encoding='latin-1')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Uz3PxJZ6E7"
      },
      "source": [
        "### 2. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv_yGMvFbZtL"
      },
      "source": [
        "# We can determine the size of our dataset\n",
        "# ---\n",
        "#\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDtEQCn6oAwJ"
      },
      "source": [
        "Seems this dataset will need some data cleaning i.e. columns. We also don't need some columns to perform create our model. We will drop those columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvsnLPXTZ8P0"
      },
      "source": [
        "### 3. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlHYpKxfHRJ4"
      },
      "source": [
        "#### Basic Data Cleaning Techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hpqmVDWbfcM"
      },
      "source": [
        "# We rename the columns for ease of referencing our columns later on\n",
        "# ---\n",
        "#\n",
        "df.columns = ['id', 'target', 't_id', 'created_at', 'query', 'user', 'text']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HSGdDX3bjUB"
      },
      "source": [
        "# We retain the relevant columns by dropping the columns we don't need \n",
        "# for creating a sentiment analysis model. \n",
        "# ---\n",
        "#\n",
        "df = df.drop(['id', 't_id', 'created_at', 'query', 'user'], axis = 1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgnEEnresgJl"
      },
      "source": [
        "# Understanding the distribution of target\n",
        "# ---\n",
        "#\n",
        "df.target.value_counts() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8riNiGfupWL"
      },
      "source": [
        "# Let's determine whether our columns have the right data types\n",
        "# ---\n",
        "#\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns54XoFrVcLp"
      },
      "source": [
        "# What values are in our target variable?\n",
        "# ---\n",
        "#\n",
        "df.target.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYkmFOBVtMC"
      },
      "source": [
        "These are the two classes to which each document (text) belongs. The target value 0 means a text with a negative sentiment, while that of 4 means a text with a positive sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6JoJc4Nvz1S"
      },
      "source": [
        "# Let's check for missing values \n",
        "# ---\n",
        "# \n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxpcyoUfoy9s"
      },
      "source": [
        "We don't have any missing values, so we are good to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BdB9m4_yK-1"
      },
      "source": [
        "#### Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8BW53FUm41_"
      },
      "source": [
        "# Text Cleaning: Removing all urls/links\n",
        "# ---\n",
        "# \n",
        "df['text'] =  df['text'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+','', str(x)))\n",
        "df[['text']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR9gL4v8m9mV"
      },
      "source": [
        "# Text Cleaning: Removing @ and # characters or replace them with space\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cTS7iaXdAQz"
      },
      "source": [
        "# Text Cleaning: Conversion to lowercase\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKO79Jh_wWD_"
      },
      "source": [
        "# Text Cleaning: Splitting concatenated words\n",
        "# ---\n",
        "# Performing this step will take few minutes...\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "# \n",
        "\n",
        "# Installing wordnija and textblob\n",
        "# ---\n",
        "#\n",
        "\n",
        "\n",
        "# Importing those libraries\n",
        "# ---\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv56Jsl6z_Ew"
      },
      "source": [
        "# Performing the split\n",
        "# ---\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edYEOU8Tc6sJ"
      },
      "source": [
        "# Text Cleaning: Removing punctuation characters\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxVb9-dJ9zN5"
      },
      "source": [
        "# Text Cleaning: Removing stop words\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "# \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37RyhNK-q6Df"
      },
      "source": [
        "# Text Cleaning: Lemmatization\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# For lemmatization, we will need to download wordnet\n",
        "#\n",
        "\n",
        "\n",
        "# Lemmatizing our text\n",
        "# ---\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t8mVoSYIAAy"
      },
      "source": [
        "We won't remove numerics because we could loose meaning of our text if we lost the numerics. We could also further prepare our text by performing spelling correction but this is a resource intensive process that we will skip for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve5fMuCicKkt"
      },
      "source": [
        "#### Feature Engineering Techniques "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFwaX_bucVar"
      },
      "source": [
        "# Feature Construction: Length of tweet\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE70M5mYceto"
      },
      "source": [
        "# Feature Construction: Word count \n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yjOYYC4cfji"
      },
      "source": [
        "# Feature Construction: Word density (Average no. of words / tweet)\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoSoEEctcgRq"
      },
      "source": [
        "# Feature Construction: Noun count\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n",
        "# First, we will download the punkt and the averaged_perceptron_tagger into our notebook environment. \n",
        "# which will allow us to find the part of speech tags.\n",
        "# ---\n",
        "#\n",
        "\n",
        "\n",
        "# We create the function to check and get the part of speech tag count of a words in a given sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxzVY6q60V1X"
      },
      "source": [
        "# Noun Count\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0hHkQb_cfNI"
      },
      "source": [
        "# Feature Construction: Verb count\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7foa2jELcdc7"
      },
      "source": [
        "# Feature Construction: Adjective count / Tweet\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smNLu-KJcdMT"
      },
      "source": [
        "# Feature Construction: Adverb count / Tweet\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRjeP3R2dDWA"
      },
      "source": [
        "# Feature Construction: Pronoun \n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1ab-9bOdEAT"
      },
      "source": [
        "# Feature Construction: Subjectivity\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEt889AByeHq"
      },
      "source": [
        "# Feature Construction: Polarity\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geXOqgRLdCBL"
      },
      "source": [
        "# Feature Construction: Word Level N-Gram TF-IDF Feature \n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C77ntLfXdTAa"
      },
      "source": [
        "# Feature Construction: Character Level N-Gram TF-IDF Feature\n",
        "# ---\n",
        "# YOUR CODE GOES BELOW\n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75PUieNPl6j5"
      },
      "source": [
        "# Let's prepare the constructed features for modeling\n",
        "# ---\n",
        "#\n",
        "X_metadata = np.array(df.iloc[:, 2:12])\n",
        "X_metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn_kBCljx6PS"
      },
      "source": [
        "# We combine our two tfidf (sparse) matrices and X_metadata\n",
        "# ---\n",
        "#\n",
        "X = scipy.sparse.hstack([df_word_vect, df_char_vect,  X_metadata])\n",
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL7etsSY8cfy"
      },
      "source": [
        "# Getting our response variable\n",
        "# ---\n",
        "#\n",
        "y = np.array(df.iloc[:, 0])\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_iOhAPnaERN"
      },
      "source": [
        "### 4. Data Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKyd7Uwl-Vr"
      },
      "source": [
        "During this step, we will use machine learning algorithms to train and test our sentiment analysis models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "533B2cK_Ey3Z"
      },
      "source": [
        "# Splitting our data\n",
        "# ---\n",
        "#\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx3rCuu6ddht"
      },
      "source": [
        "# Fitting our model\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Importing the algorithms\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "nb_classifier = MultinomialNB() \n",
        "lr_classifier = LogisticRegression(max_iter=1000) \n",
        "\n",
        "# Training our model\n",
        "nb_classifier.fit(X_train, y_train) \n",
        "lr_classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grL4WhdTYu5g"
      },
      "source": [
        "# Making predictions\n",
        "# ---\n",
        "#\n",
        "y_predict_nb = nb_classifier.predict(X_test) \n",
        "y_predict_lr = lr_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyb3cihzdlKX"
      },
      "source": [
        "# Evaluating the Models\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Accuracy scores\n",
        "# ---\n",
        "#\n",
        "print(\"Naive Bayes Classifier:\\n\", accuracy_score(y_test, y_predict_nb)) \n",
        "print(\"Logistic Regression Classifier: \\n\", accuracy_score(y_test, y_predict_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obyOpb9uD9IM"
      },
      "source": [
        "# Confusion matrices\n",
        "# ---\n",
        "# \n",
        "print(\"Naive Bayes Classifier: \\n\", confusion_matrix(y_test, y_predict_nb)) \n",
        "print(\"Logistic Regression Classifier: \\n\", confusion_matrix(y_test, y_predict_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFdanMG4D6Dn"
      },
      "source": [
        "# Classification Reports\n",
        "# ---\n",
        "#\n",
        "print(\"Naive Bayes Classifier: \\n\", classification_report(y_test, y_predict_nb)) \n",
        "print(\"Logistic Regression Classifier: \\n\", classification_report(y_test, y_predict_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxETCaYyOyHv"
      },
      "source": [
        "**Evaluation our Models**\n",
        "\n",
        "* **Accuracy:** the percentage of texts that were assigned the correct topic.\n",
        "* **Precision:** the percentage of texts the classifier classified correctly out of the total number of texts it predicted for each topic\n",
        "* **Recall:** the percentage of texts the model predicted for each topic out of the total number of texts it should have predicted for that topic.\n",
        "* **F1 Score:** the average of both precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbisAqRswA80"
      },
      "source": [
        "To improve our model, we can try perfoming other text processing techniques that would better prepare our data for fitting our model. We can also use different vectorizing techniques, implement other machine learning models and perform hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWiaaYPCnsiC"
      },
      "source": [
        "### 5. Recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3DQO1yFnvx3"
      },
      "source": [
        "Our best model had an accuracy of 73.25% and use it for classifying newer tweets. We can improve this performance by performing hyperparameter tuning and feature engineering methods. "
      ]
    }
  ]
}